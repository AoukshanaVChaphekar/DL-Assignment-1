{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZE3U_NS7KiB",
        "outputId": "b5d19c03-33e4-4edd-c448-4c5376440392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.11-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=471e083199d20082d868fb0061562ebfd1bef1e45772e6337f0c4960f08da462\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.13.11\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEN8OmfA6oZb"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import wandb\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "import copy\n",
        "\n",
        "class FeedForward:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # defining the default parameters\n",
        "        self.parameters = {\n",
        "            \"wandb_project\": \"DL Final Assignment 1\",\n",
        "            \"wandb_entity\": \"cs22m019\",\n",
        "            \"dataset\": \"fashion_mnist\",\n",
        "            \"epochs\": 5,\n",
        "            \"batch_size\": 32,\n",
        "            \"loss\": \"mean_squared_error\",\n",
        "            \"optimizer\": \"gd\",\n",
        "            \"learning_rate\": 0.1,\n",
        "            \"momentum\": 0.01,\n",
        "            \"beta\": 0.5,\n",
        "            \"beta1\": 0.5,\n",
        "            \"beta2\": 0.5,\n",
        "            \"epsilon\": 0.000001,\n",
        "            \"weight_decay\": 0.0,\n",
        "            \"weight_init\": \"random\",\n",
        "            \"num_layers\": 3,\n",
        "            \"hidden_size\": 128,\n",
        "            \"activation\": \"sigmoid\",\n",
        "            \"output_function\": \"softmax\"\n",
        "        }\n",
        "\n",
        "        # updating the parameters to the parameters given in command line\n",
        "        # self.update_parameters()\n",
        "\n",
        "       \n",
        "        # loading training and test data from fashion_mnist dataset or mnist dataset\n",
        "        if (self.parameters[\"dataset\"] == \"fashion_mnist\"):\n",
        "            (self.x_train, self.y_train), (self.x_test,self.y_test) = fashion_mnist.load_data()\n",
        "        else:\n",
        "            (self.x_train, self.y_train), (self.x_test,self.y_test) = mnist.load_data()\n",
        "\n",
        "        # normalizing data points\n",
        "        self.x_train = self.x_train / 255\n",
        "        self.x_test = self.x_test / 255\n",
        "\n",
        "        # computing number of samples in training and test data\n",
        "        self.train_n_samples = self.x_train.shape[0]\n",
        "        self.test_n_samples = self.x_test.shape[0]\n",
        "\n",
        "        # spiltting the data -> 90% train,10% test \n",
        "        idx = np.random.permutation(self.train_n_samples)\n",
        "        self.x_train = self.x_train[idx]\n",
        "        self.y_train = self.y_train[idx]\n",
        "\n",
        "        self.x_validate = self.x_train[: self.train_n_samples // 10]\n",
        "        self.y_validate = self.y_train[: self.train_n_samples // 10]\n",
        "\n",
        "        self.x_train = self.x_train[self.train_n_samples // 10:]\n",
        "        self.y_train = self.y_train[self.train_n_samples // 10:]\n",
        "\n",
        "        self.train_n_samples = self.x_train.shape[0]\n",
        "\n",
        "        # list of label titles -> actual output\n",
        "        self.title = [\"T-shirt/top\", \"Trouser\", \"PullOver\", \"Dress\",\n",
        "                      \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
        "        self.no_of_label = len(self.title)\n",
        "\n",
        "        # setting the class variables\n",
        "        self.epoch = self.parameters[\"epochs\"]\n",
        "        self.batch_size = self.parameters[\"batch_size\"]\n",
        "        self.lossFunction = self.parameters[\"loss\"]\n",
        "        self.optimizer = self.parameters[\"optimizer\"]\n",
        "        self.learningRate = self.parameters[\"learning_rate\"]\n",
        "        self.weightInitialization = self.parameters[\"weight_init\"]\n",
        "        self.L = self.parameters[\"num_layers\"] + 1\n",
        "        self.hl = self.parameters[\"num_layers\"]\n",
        "        self.nnl = self.parameters[\"hidden_size\"]\n",
        "        self.activationFunction = self.parameters[\"activation\"]\n",
        "        self.outputFunction = self.parameters[\"output_function\"]\n",
        "        self.weight_decay = self.parameters[\"weight_decay\"]\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.k = len(self.title)\n",
        "        self.d = self.x_train.shape[1] * self.x_train.shape[2]\n",
        "        self.n = self.train_n_samples\n",
        "        self.weights = {}\n",
        "        self.bias = {}\n",
        "        self.wHistory = {}\n",
        "        self.bHistory = {}\n",
        "        self.wMomentum = {}\n",
        "        self.bMomentum = {}\n",
        "        self.prev_wHistory = {}\n",
        "        self.prev_bHistory = {}\n",
        "        self.pre_activation = {}\n",
        "        self.post_activation = {}\n",
        "        \n",
        "    # updates the default parameters with the paramters given in command line\n",
        "    def update_parameters(self):\n",
        "\n",
        "        parser = argparse.ArgumentParser(description='DL Assignment 1 Parser')\n",
        "\n",
        "        parser.add_argument('-wp', '--wandb_project',\n",
        "                            type=str, metavar='', help='wandb project')\n",
        "        parser.add_argument('-we', '--wandb_entity', type=str,\n",
        "                            metavar='', help='wandb entity')\n",
        "        parser.add_argument('-d', '--dataset', type=str,\n",
        "                            metavar='', help='dataset')\n",
        "        parser.add_argument('-e', '--epochs', type=int,\n",
        "                            metavar='', help='epochs')\n",
        "        parser.add_argument('-b', '--batch_size', type=int,\n",
        "                            metavar='', help='batch size')\n",
        "        parser.add_argument('-l', '--loss', type=str, \n",
        "                            metavar='', help='loss')\n",
        "        parser.add_argument('-o', '--optimizer', type=str,\n",
        "                            metavar='', help='optimizer')\n",
        "        parser.add_argument('-lr', '--learning_rate',\n",
        "                            type=float, metavar='', help='learning rate')\n",
        "        parser.add_argument('-m', '--momentum', type=float,\n",
        "                            metavar='', help='momentum')\n",
        "        parser.add_argument('-beta', '--beta', type=float,\n",
        "                            metavar='', help='beta')\n",
        "        parser.add_argument('-beta1', '--beta1', type=float,\n",
        "                            metavar='', help='beta1')\n",
        "        parser.add_argument('-beta2', '--beta2', type=float,\n",
        "                            metavar='', help='beta2')\n",
        "        parser.add_argument('-eps', '--epsilon', type=float,\n",
        "                            metavar='', help='epsilon')\n",
        "        parser.add_argument('-w_d', '--weight_decay',\n",
        "                            type=float, metavar='', help='weight decay')\n",
        "        parser.add_argument('-w_i', '--weight_init', type=str,\n",
        "                            metavar='', help='weight init')\n",
        "        parser.add_argument('-nhl', '--num_layers', type=int,\n",
        "                            metavar='', help='num layers')\n",
        "        parser.add_argument('-sz', '--hidden_size', type=int,\n",
        "                            metavar='', help='hidden size')\n",
        "        parser.add_argument('-a', '--activation', type=str,\n",
        "                            metavar='', help='activation')\n",
        "        parser.add_argument('-of', '--output_function',\n",
        "                            type=str, metavar='', help='output function')\n",
        "        args = parser.parse_args()\n",
        "\n",
        "        if (args.wandb_project != None):\n",
        "            self.parameters[\"wandb_project\"] = args.wandb_project\n",
        "        if (args.wandb_entity != None):\n",
        "            self.parameters[\"wandb_entity\"] = args.wandb_entity\n",
        "        if (args.dataset != None):\n",
        "            self.parameters[\"dataset\"] = args.dataset\n",
        "        if (args.epochs != None):\n",
        "            self.parameters[\"epochs\"] = args.epochs\n",
        "        if (args.batch_size != None):\n",
        "            self.parameters[\"batch_size\"] = args.batch_size\n",
        "        if (args.loss != None):\n",
        "            self.parameters[\"loss\"] = args.loss\n",
        "        if (args.optimizer != None):\n",
        "            self.parameters[\"optimizer\"] = args.optimizer\n",
        "        if (args.learning_rate != None):\n",
        "            self.parameters[\"learning_rate\"] = args.learning_rate\n",
        "        if (args.momentum != None):\n",
        "            self.parameters[\"momentum\"] = args.momentum\n",
        "        if (args.beta != None):\n",
        "            self.parameters[\"beta\"] = args.beta\n",
        "        if (args.beta1 != None):\n",
        "            self.parameters[\"beta1\"] = args.beta1\n",
        "        if (args.beta2 != None):\n",
        "            self.parameters[\"beta2\"] = args.beta2\n",
        "        if (args.epsilon != None):\n",
        "            self.parameters[\"epsilon\"] = args.epsilon\n",
        "        if (args.weight_decay != None):\n",
        "            self.parameters[\"weight_decay\"] = args.weight_decay\n",
        "        if (args.weight_init != None):\n",
        "            self.parameters[\"weight_init\"] = args.weight_init\n",
        "        if (args.num_layers != None):\n",
        "            self.parameters[\"num_layers\"] = args.num_layers\n",
        "        if (args.hidden_size != None):\n",
        "            self.parameters[\"hidden_size\"] = args.hidden_size\n",
        "        if (args.activation != None):\n",
        "            self.parameters[\"activation\"] = args.activation\n",
        "\n",
        "    # function to initialize weights and bias based on type -> random or Xavier initialization\n",
        "    def weightsAndBiasInitializer(self):\n",
        "        if self.weightInitialization == \"Xavier\":\n",
        "\n",
        "            # first and last matrix \n",
        "            self.weights[\"w1\"] = np.random.uniform(-np.sqrt(6 / (self.nnl + self.d)), np.sqrt(6 / (self.nnl + self.d)), (self.nnl, self.d))\n",
        "            self.weights[\"w\" + str(self.L)] = np.random.uniform(-np.sqrt(6 / (self.k + self.nnl)),np.sqrt(6 / (self.k + self.nnl)), (self.k, self.nnl))\n",
        "\n",
        "            # Intermediate Matrices\n",
        "            for i in range(2, self.L):\n",
        "                self.weights[\"w\" + str(i)] = np.random.uniform(-np.sqrt(6 / (self.nnl + self.nnl)), np.sqrt(6 / (self.nnl + self.nnl)), (self.nnl, self.nnl))\n",
        "\n",
        "            # Last Vector\n",
        "            self.bias[\"b\" + str(self.L)] = np.random.uniform(-np.sqrt(6 / (self.k + 1)),np.sqrt(6 / (self.k + 1)), (self.k))\n",
        "\n",
        "            for i in range(1, self.L):\n",
        "                self.bias[\"b\" + str(i)] = np.random.uniform(-np.sqrt(6 / (self.nnl + 1)),np.sqrt(6 / (self.nnl + 1)), (self.nnl))\n",
        "\n",
        "        if self.weightInitialization == \"random\":\n",
        "            # initailzation of weights\n",
        "            '''\n",
        "                  W1 = (d,nnl)\n",
        "                  W2,..,W(L - 1) = (nnl,nnl)\n",
        "                  WL = (k,nnl)\n",
        "            '''\n",
        "            w1 = np.random.normal(0, 0.5, size=(self.nnl, self.d))\n",
        "            self.weights[\"w1\"] = w1\n",
        "            for i in range(2, self.L):\n",
        "                self.weights[\"w\" + str(i)] = np.random.normal(0,0.5, size=(self.nnl, self.nnl))\n",
        "            self.weights[\"w\" + str(self.L)] = np.random.normal(0,0.5, size=(self.k, self.nnl))\n",
        "\n",
        "            # initialization of bias\n",
        "            for i in range(1, self.L):\n",
        "                self.bias[\"b\" + str(i)] = np.random.normal(0,0.5, size=(self.nnl))\n",
        "            self.bias[\"b\" + str(self.L)] = np.random.normal(0,0.5, size=(self.k))\n",
        "\n",
        "    # function to initialize momentum for weights and bias\n",
        "    def momentumInitializer(self):\n",
        "\n",
        "        # initializing momentum for weights\n",
        "        w1 = np.zeros((self.nnl, self.d))\n",
        "        self.wMomentum[\"w1\"] = w1\n",
        "        for i in range(2, self.L):\n",
        "            self.wMomentum[\"w\" + str(i)] = np.zeros((self.nnl, self.nnl))\n",
        "        self.wMomentum[\"w\" + str(self.L)] = np.zeros((self.k, self.nnl))\n",
        "\n",
        "        # initializing momentum for bais\n",
        "        for i in range(1, self.L):\n",
        "            self.bMomentum[\"b\" + str(i)] = np.zeros((self.nnl))\n",
        "        self.bMomentum[\"b\" + str(self.L)] = np.zeros((self.k))\n",
        "\n",
        "    # function to initialize history for weights and bias\n",
        "    def historyInitializer(self):\n",
        "\n",
        "        # initializing history for weights\n",
        "        w1 = np.zeros((self.nnl, self.d))\n",
        "        self.wHistory[\"w1\"] = w1\n",
        "        for i in range(2, self.L):\n",
        "            self.wHistory[\"w\" + str(i)] = np.zeros((self.nnl, self.nnl))\n",
        "        self.wHistory[\"w\" + str(self.L)] = np.zeros((self.k, self.nnl))\n",
        "\n",
        "        # initializing history for bais\n",
        "        for i in range(1, self.L):\n",
        "            self.bHistory[\"b\" + str(i)] = np.zeros((self.nnl))\n",
        "        self.bHistory[\"b\" + str(self.L)] = np.zeros((self.k))\n",
        "\n",
        "    # function used to implement different activation functions\n",
        "    def activation_func(self, vector):\n",
        "        if self.activationFunction == \"sigmoid\":\n",
        "            return 1.0 / (1 + np.exp(-(vector)))\n",
        "        \n",
        "        if self.activationFunction == \"tanh\":\n",
        "            return np.tanh(vector)\n",
        "        \n",
        "        if self.activationFunction == \"ReLU\":\n",
        "            return np.maximum(0,vector)\n",
        "\n",
        "    # function used to implement different output functions\n",
        "    def output_func(self, vector):\n",
        "        if self.outputFunction == \"softmax\":\n",
        "\n",
        "            vector = vector - vector[np.argmax(vector)]\n",
        "\n",
        "            return np.exp(vector) / np.sum(np.exp(vector))\n",
        "\n",
        "    # function generating one-hot vector\n",
        "    def oneHotVector(self, size, index):\n",
        "        oneHot = np.zeros(size)\n",
        "        oneHot[index] = 1.0\n",
        "        return oneHot\n",
        "\n",
        "    # function returning the differentiation of activation function\n",
        "    def differentiation(self, vector):\n",
        "\n",
        "        if self.activationFunction == \"sigmoid\":\n",
        "            return (1.0 / (1 + np.exp(-(vector)))) * (1 - 1.0 / (1 + np.exp(-(vector))))\n",
        "\n",
        "        if self.activationFunction == \"tanh\":\n",
        "            return 1 - (np.tanh(vector)) ** 2\n",
        "\n",
        "        if self.activationFunction == \"ReLU\":\n",
        "            t = np.maximum(0,vector)\n",
        "            t[t > 0] = 1\n",
        "            return t\n",
        "\n",
        "    # regularization\n",
        "    def regularize(self):\n",
        "        reg_term = 0\n",
        "        validation_size = self.y_validate.shape[0]\n",
        "        \n",
        "        for (key,value) in self.weights.items():\n",
        "          reg_term += (np.sum(self.weights[key] ** 2))\n",
        "        reg_term = (self.weight_decay / (2 * validation_size)) * reg_term\n",
        "\n",
        "        return reg_term\n",
        "\n",
        "    # function returning the loss function value\n",
        "    def loss_function(self, y_predicted, index):\n",
        "\n",
        "        if self.lossFunction == \"cross_entropy\":\n",
        "            t = 1e-8\n",
        "            return (-1)*np.log(y_predicted[index] + t)\n",
        "\n",
        "        if self.lossFunction == \"mean_squared_error\":\n",
        "            y = self.oneHotVector(size=self.no_of_label, index=index)\n",
        "            return np.sum((y_predicted - y) ** 2)\n",
        "\n",
        "    # forward propagation - computes pre_activation vector,post_activation vector for each layer and predicts y at last layer\n",
        "    def forward_propagation(self, input, index):\n",
        "\n",
        "        # Populating pre_activation and post_activation vectors to dictionary in each layer for input[index]\n",
        "        for k in range(1, self.L):\n",
        "\n",
        "            # for first layer,post activation will be input\n",
        "            if (k == 1):\n",
        "                ''' flattening the input: \n",
        "                    -input(60000,28,28)\n",
        "                    -input[index] size = (28,28)\n",
        "                    -flattening input[index] gives size (784,1) = (d,1) where d is dimension of input\n",
        "                    post_activation[h0] size = (d,1)\n",
        "                    bias[b1] size = (nnl,1)\n",
        "                    weights[w1] size = (nnl,d)\n",
        "                    Therefore we get pre_activation[a1] size = (nnl,1) for all layer except last layer\n",
        "                '''\n",
        "                self.post_activation[\"h\" + str(k - 1)] = input[index].flatten()\n",
        "\n",
        "            # computing a(k) = b(k) + w(k)*h(k - 1) for each input[index]\n",
        "            self.pre_activation[\"a\" + str(k)] = self.bias[\"b\" + str(k)] + np.dot(self.weights[\"w\" + str(k)], self.post_activation[\"h\" + str(k - 1)])\n",
        "           \n",
        "            # computing h(k) = g(a(k)) where g is activation function\n",
        "            self.post_activation[\"h\" + str(k)] = self.activation_func(self.pre_activation[\"a\" + str(k)])\n",
        "\n",
        "        # computing pre_activation for last layer\n",
        "        self.pre_activation[\"a\" + str(self.L)] = self.bias[\"b\" + str(self.L)] + np.dot(self.weights[\"w\" + str(self.L)], self.post_activation[\"h\" + str(self.L - 1)])\n",
        "\n",
        "        # prediction y (y_hat) = O(a(L)) where O is output function\n",
        "        self.post_activation[\"h\" +str(self.L)] = self.output_func(self.pre_activation[\"a\" + str(self.L)])\n",
        "\n",
        "    # performs back propagation and returns gradients of weights and bias\n",
        "    def backward_propagation(self, index, actual_y):\n",
        "\n",
        "        grad_pre_activation = {}\n",
        "        grad_post_activation = {}\n",
        "        grad_weights = {}\n",
        "        grad_bias = {}\n",
        "\n",
        "        predicted_y = self.post_activation[\"h\" + str(self.L)]\n",
        "\n",
        "        # Computing output gradient\n",
        "        one_hot_vector = self.oneHotVector(self.no_of_label, actual_y[index])\n",
        "        if self.lossFunction == \"cross_entropy\" :\n",
        "          grad_pre_activation[\"a\" + str(self.L)] = (predicted_y - one_hot_vector)\n",
        "        else :\n",
        "          grad_pre_activation[\"a\" + str(self.L)] = -2 * (one_hot_vector - predicted_y) * (predicted_y * (np.ones(self.no_of_label) - predicted_y))\n",
        "       \n",
        "        \n",
        "        k = self.L\n",
        "        while k > 0:\n",
        "\n",
        "            # Computing gradient w.r.t parameters - weight and bais\n",
        "            '''\n",
        "              np.reshape(grad_pre_activation[\"a\" + str(L)],(-1,1)) = (k,1)\n",
        "              np.reshape(post_activation[\"h\" + str(L - 1)],(1,-1)) = (1,nnl)\n",
        "            '''\n",
        "            grad_weights[\"w\" + str(k)] = np.dot(np.reshape(grad_pre_activation[\"a\" + str(k)], (-1, 1)), np.reshape(self.post_activation[\"h\" + str(k - 1)], (1, -1)))\n",
        "            grad_bias[\"b\" + str(k)] = grad_pre_activation[\"a\" + str(k)]\n",
        "\n",
        "            if k != 1:\n",
        "                # Computing gradient w.r.t layer below (post_activation)\n",
        "                grad_post_activation[\"h\" + str(k - 1)] = np.dot(self.weights[\"w\" + str(k)].T, np.reshape(grad_pre_activation[\"a\" + str(k)], (-1, 1))).flatten()\n",
        "\n",
        "                # Computing gradient w.r.t layer below (pre_activation)\n",
        "                g_dash = self.differentiation(self.pre_activation[\"a\" + str(k - 1)])\n",
        "                grad_pre_activation[\"a\" +str(k - 1)] = grad_post_activation[\"h\" + str(k - 1)] * g_dash\n",
        "\n",
        "            k = k - 1\n",
        "        return grad_weights, grad_bias\n",
        "\n",
        "    # function to make accumalated gradients zero\n",
        "    def make_accumalate_zero(self):\n",
        "\n",
        "        acc_grad_weights = {}\n",
        "        acc_grad_bias = {}\n",
        "\n",
        "        # accumalated weights are set to zero\n",
        "        acc_grad_weights[\"w1\"] = np.zeros((self.nnl, self.d))\n",
        "        for i in range(2, self.L):\n",
        "            acc_grad_weights[\"w\" + str(i)] = np.zeros((self.nnl, self.nnl))\n",
        "        acc_grad_weights[\"w\" + str(self.L)] = np.zeros((self.k, self.nnl))\n",
        "\n",
        "        # accumalated bias are set to zero\n",
        "        for i in range(1, self.L):\n",
        "            acc_grad_bias[\"b\" + str(i)] = np.zeros((self.nnl))\n",
        "        acc_grad_bias[\"b\" + str(self.L)] = np.zeros((self.k))\n",
        "\n",
        "        return acc_grad_weights, acc_grad_bias\n",
        "\n",
        "    # runs stochastic gradient descent for one epoch\n",
        "    def oneEpochSGD(self, epoch):\n",
        "        ''' Executes A Single Epoch for Stochastic Gradient Descent Algorithm.\n",
        "            Returns the training loss,training accuracy,validaiton loss and validation accuracy,averaged over all points. '''\n",
        "        \n",
        "        n = self.train_n_samples\n",
        "        \n",
        "        # randomizing batches\n",
        "        idx = np.random.permutation(self.train_n_samples)\n",
        "        self.x_train = self.x_train[idx]\n",
        "        self.y_train = self.y_train[idx]\n",
        "\n",
        "        input = self.x_train\n",
        "        actual_y = self.y_train\n",
        "\n",
        "        # total Loss for epoch\n",
        "        loss_input = 0\n",
        "        count = 0\n",
        "\n",
        "        # execute one epoch for all datapoints in train set\n",
        "        for index in range(n):\n",
        "\n",
        "            # perform forward propagation\n",
        "            self.forward_propagation(input, index)\n",
        "            predicted_y = self.post_activation[\"h\" + str(self.L)]\n",
        "\n",
        "            # compute loss\n",
        "            loss_input += self.loss_function(predicted_y, actual_y[index])\n",
        "\n",
        "            # perform backward propagation\n",
        "            grad_weights, grad_bias = self.backward_propagation(index, actual_y)\n",
        "\n",
        "            # compute the number of datapoints which are correctly classified\n",
        "            indexWithMaxProb = np.argmax(predicted_y)\n",
        "            if (actual_y[index] == (indexWithMaxProb)):\n",
        "                count = count + 1\n",
        "\n",
        "            # update weights and bias if the number of datapoints in batch_size are divisble by batch_size\n",
        "            if ((index + 1) % self.batch_size == 0):\n",
        "                # update weights\n",
        "                for (key, value) in self.weights.items():\n",
        "                    self.weights[key] = self.weights[key] - ((self.learningRate / self.batch_size) * grad_weights[key])\n",
        "\n",
        "                # update bias\n",
        "                for (key, value) in self.bias.items():\n",
        "                    self.bias[key] = self.bias[key] - ((self.learningRate / self.batch_size) * grad_bias[key])\n",
        "\n",
        "        # if the number of datapoints in batch is not divisible by batch_size update weights and bias \n",
        "        if n % self.batch_size != 0:\n",
        "            # update weights\n",
        "            for (key, value) in self.weights.items():\n",
        "                self.weights[key] = self.weights[key] - ((self.learningRate / self.batch_size) * grad_weights[key])\n",
        "\n",
        "            # update bias\n",
        "            for (key, value) in self.bias.items():\n",
        "                self.bias[key] = self.bias[key] - ((self.learningRate / self.batch_size) * grad_bias[key])\n",
        "\n",
        "        # compute trainAccuracy,trainLoss averaged over train size\n",
        "        trainAccuracy = count / n\n",
        "        trainLoss = loss_input / n + self.regularize()\n",
        "\n",
        "        # compute validationAccuracy,validationLoss avergaed over test size \n",
        "        validationLoss, validationAccuracy = self.computeTestLossAndAccuracy()\n",
        "        \n",
        "        return trainLoss, trainAccuracy, validationLoss, validationAccuracy\n",
        "\n",
        "    # runs momentum gradient descent for one epoch\n",
        "    def oneEpochMOMENTUM(self, epoch):\n",
        "        ''' Executes A Single Epoch for Momentum Gradient Descent Algorithm.\n",
        "            Returns the training loss,training accuracy,validaiton loss and validation accuracy,averaged over all points. '''\n",
        "\n",
        "        n = self.train_n_samples\n",
        "        \n",
        "        # randomizing batches\n",
        "        idx = np.random.permutation(self.train_n_samples)\n",
        "        self.x_train = self.x_train[idx]\n",
        "        self.y_train = self.y_train[idx]\n",
        "\n",
        "        input = self.x_train\n",
        "        actual_y = self.y_train\n",
        "\n",
        "        # maintaining previous history for weights and bias\n",
        "        self.prev_wHistory, self.prev_bHistory = self.wHistory, self.bHistory\n",
        "\n",
        "        # Total Loss for epoch\n",
        "        loss_input = 0\n",
        "        count = 0  \n",
        "        beta = self.parameters[\"momentum\"]\n",
        "\n",
        "        # set accumalated gradients to zero\n",
        "        acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "\n",
        "        # execute one epoch for all datapoints in train set\n",
        "        for index in range(n):\n",
        "\n",
        "            # perform forward propagation\n",
        "            self.forward_propagation(input, index)\n",
        "            predicted_y = self.post_activation[\"h\" + str(self.L)]\n",
        "\n",
        "            # compute loss\n",
        "            loss_input += self.loss_function(predicted_y, actual_y[index])\n",
        "\n",
        "            # perform backward propagation\n",
        "            grad_weights, grad_bias = self.backward_propagation(\n",
        "                index, actual_y)\n",
        "\n",
        "            # compute the number of datapoints which are correctly classified\n",
        "            indexWithMaxProb = np.argmax(predicted_y)\n",
        "            if (actual_y[index] == (indexWithMaxProb)):\n",
        "                count = count + 1\n",
        "\n",
        "            # accumulate grad_weights and grad_bais for each input\n",
        "            for (key, value) in grad_weights.items():\n",
        "                acc_grad_weights[key] = acc_grad_weights[key] + grad_weights[key]\n",
        "\n",
        "            for (key, value) in grad_bias.items():\n",
        "                acc_grad_bias[key] = acc_grad_bias[key] + grad_bias[key]\n",
        "\n",
        "\n",
        "            # update weights and bias if the number of datapoints in batch_size are divisble by batch_size\n",
        "            if ((index + 1) % self.batch_size == 0):\n",
        "\n",
        "                # update weight history\n",
        "                for (key, value) in self.wHistory.items():\n",
        "                    self.wHistory[key] = beta * self.prev_wHistory[key] + \\\n",
        "                        ((self.learningRate / self.batch_size) * acc_grad_weights[key])\n",
        "\n",
        "                # update bias history\n",
        "                for (key, value) in self.bHistory.items():\n",
        "                    self.bHistory[key] = beta * self.prev_bHistory[key] + \\\n",
        "                        ((self.learningRate / self.batch_size) * acc_grad_bias[key])\n",
        "\n",
        "                # update weights\n",
        "                for (key, value) in self.weights.items():\n",
        "                    self.weights[key] = self.weights[key] - self.wHistory[key]\n",
        "\n",
        "                # update bias\n",
        "                for (key, value) in self.bias.items():\n",
        "                    self.bias[key] = self.bias[key] - self.bHistory[key]\n",
        "\n",
        "                # updating histroy for weights and bias\n",
        "                self.prev_wHistory = self.wHistory\n",
        "                self.prev_bHistory = self.bHistory\n",
        "\n",
        "                # set accumalated gradients to zero\n",
        "                acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "\n",
        "            # if the number of datapoints in batch is not divisible by batch_size update weights and bias \n",
        "        if n % self.batch_size != 0:\n",
        "            \n",
        "            # update weight history\n",
        "            for (key, value) in self.wHistory.items():\n",
        "                self.wHistory[key] = beta * self.prev_wHistory[key] + \\\n",
        "                    ((self.learningRate / self.batch_size) * acc_grad_weights[key])\n",
        "\n",
        "            # update bias history\n",
        "            for (key, value) in self.bHistory.items():\n",
        "                self.bHistory[key] = beta * self.prev_bHistory[key] + \\\n",
        "                    ((self.learningRate / self.batch_size) * acc_grad_bias[key])\n",
        "\n",
        "            # update weights\n",
        "            for (key, value) in self.weights.items():\n",
        "                self.weights[key] = self.weights[key] - self.wHistory[key]\n",
        "\n",
        "            # update bias\n",
        "            for (key, value) in self.bias.items():\n",
        "                self.bias[key] = self.bias[key] - self.bHistory[key]\n",
        "\n",
        "            # updating histroy for weights and bias\n",
        "            self.prev_wHistory = self.wHistory\n",
        "            self.prev_bHistory = self.bHistory\n",
        "\n",
        "            # set accumalated gradients to zero\n",
        "            acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "\n",
        "        # compute trainAccuracy,trainLoss averaged over train size\n",
        "        trainAccuracy = count / n\n",
        "        trainLoss = loss_input / n + self.regularize()\n",
        "\n",
        "        # compute validationAccuracy,validationLoss averaged over test size \n",
        "        validationLoss, validationAccuracy = self.computeTestLossAndAccuracy()\n",
        "        \n",
        "        return trainLoss, trainAccuracy, validationLoss, validationAccuracy\n",
        "\n",
        "    # runs nestrov accelerated gradient descent for one epoch\n",
        "    def oneEpochNAG(self, epoch):\n",
        "        ''' Executes A Single Epoch for Nesterov Accelerated Gradient Descent Algorithm.\n",
        "            Returns the training loss,training accuracy,validaiton loss and validation accuracy,averaged over all points. '''\n",
        "        \n",
        "        n = self.train_n_samples\n",
        "        \n",
        "        # randomizing batches\n",
        "        idx = np.random.permutation(self.train_n_samples)\n",
        "        self.x_train = self.x_train[idx]\n",
        "        self.y_train = self.y_train[idx]\n",
        "\n",
        "        input = self.x_train\n",
        "        actual_y = self.y_train\n",
        "\n",
        "        # maintaining previous history for weights and bias\n",
        "        self.prev_wHistory, self.prev_bHistory = self.wHistory, self.bHistory\n",
        "        \n",
        "        # Total Loss for epoch\n",
        "        loss_input = 0\n",
        "        count = 0\n",
        "        beta = self.parameters[\"momentum\"]\n",
        "\n",
        "        # set accumalated gradients to zero\n",
        "        acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "        \n",
        "        # computing partial values\n",
        "        self.partial_wHistory = {}\n",
        "        self.partial_bHistory = {}\n",
        "\n",
        "        for (key, value) in self.wHistory.items():\n",
        "            self.partial_wHistory[key] = beta * self.prev_wHistory[key]\n",
        "\n",
        "        for (key, value) in self.bHistory.items():\n",
        "            self.partial_bHistory[key] = beta * self.prev_bHistory[key]\n",
        "\n",
        "        # execute one epoch for all datapoints in train set\n",
        "        for index in range(n):\n",
        "\n",
        "            # perfrom forward propagation\n",
        "            self.forward_propagation(input, index)\n",
        "            predicted_y = self.post_activation[\"h\" + str(self.L)]\n",
        "\n",
        "            # storing weights and bias in temperory values\n",
        "            temp_weights = copy.deepcopy(self.weights)\n",
        "            temp_bias = copy.deepcopy(self.bias)\n",
        "\n",
        "            # update weights and bias\n",
        "            for (key, value) in self.weights.items():\n",
        "                self.weights[key] = self.weights[key] -  self.partial_wHistory[key]\n",
        "\n",
        "            for (key, value) in self.bias.items():\n",
        "                self.bias[key] = self.bias[key] - self.partial_bHistory[key]\n",
        "\n",
        "            # perform backward propagation\n",
        "            grad_weights, grad_bias = self.backward_propagation(index,actual_y)\n",
        "\n",
        "            # update weights and bias \n",
        "            self.weights = temp_weights\n",
        "            self.bias = temp_bias\n",
        "\n",
        "            # accumulate grad_weights and grad_bais for each input\n",
        "            for (key, value) in grad_weights.items():\n",
        "                acc_grad_weights[key] = acc_grad_weights[key] + grad_weights[key]\n",
        "\n",
        "            for (key, value) in grad_bias.items():\n",
        "                acc_grad_bias[key] = acc_grad_bias[key] + grad_bias[key]\n",
        "\n",
        "            # compute loss\n",
        "            loss_input += self.loss_function(predicted_y, self.y_train[index])\n",
        "\n",
        "            # compute the number of datapoints which are correctly classified\n",
        "            indexWithMaxProb = np.argmax(predicted_y)\n",
        "            if (actual_y[index] == (indexWithMaxProb)):\n",
        "                count = count + 1\n",
        "\n",
        "            # update weights and bias if the number of datapoints in batch_size are divisble by batch_size\n",
        "            if ((index + 1) % self.batch_size == 0):\n",
        "    \n",
        "                # update weight history\n",
        "                for (key, value) in self.wHistory.items():\n",
        "                    self.wHistory[key] = beta * self.prev_wHistory[key] + ((self.learningRate / self.batch_size) * acc_grad_weights[key])\n",
        "\n",
        "                # update bias history\n",
        "                for (key, value) in self.bHistory.items():\n",
        "                    self.bHistory[key] = beta * self.prev_bHistory[key] + ((self.learningRate / self.batch_size) * acc_grad_bias[key])\n",
        "\n",
        "                # update weights\n",
        "                for (key, value) in self.weights.items():\n",
        "                    self.weights[key] = self.weights[key] - self.wHistory[key]\n",
        "\n",
        "                for (key, value) in self.bias.items():\n",
        "                    self.bias[key] = self.bias[key] - self.bHistory[key]\n",
        "\n",
        "                # updating histroy for weights and bias\n",
        "                self.prev_wHistory = self.wHistory\n",
        "                self.prev_bHistory = self.bHistory\n",
        "                \n",
        "                # set accumalated gradients to zero\n",
        "                acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "            \n",
        "        # if the number of datapoints in batch is not divisible by batch_size update weights and bias \n",
        "        if n % self.batch_size != 0:\n",
        "\n",
        "            # update weight history\n",
        "            for (key, value) in self.wHistory.items():\n",
        "                self.wHistory[key] = beta * self.prev_wHistory[key] + ((self.learningRate / self.batch_size) * acc_grad_weights[key])\n",
        "\n",
        "            # update bias history\n",
        "            for (key, value) in self.bHistory.items():\n",
        "                self.bHistory[key] = beta * self.prev_bHistory[key] + ((self.learningRate / self.batch_size) * acc_grad_bias[key])\n",
        "\n",
        "            # update weights\n",
        "            for (key, value) in self.weights.items():\n",
        "                self.weights[key] = self.weights[key] - self.wHistory[key]\n",
        "\n",
        "            for (key, value) in self.bias.items():\n",
        "                self.bias[key] = self.bias[key] - self.bHistory[key]\n",
        "\n",
        "            # updating histroy for weights and bias\n",
        "            self.prev_wHistory = self.wHistory\n",
        "            self.prev_bHistory = self.bHistory\n",
        "            \n",
        "            # set accumalated gradients to zero\n",
        "            acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "                \n",
        "        # compute trainAccuracy,trainLoss averaged over train size\n",
        "        trainAccuracy = count / n\n",
        "        trainLoss = loss_input / n + self.regularize()\n",
        "\n",
        "        # compute validationAccuracy,validationLoss averaged over test size \n",
        "        validationLoss, validationAccuracy = self.computeTestLossAndAccuracy()\n",
        "        \n",
        "        return trainLoss, trainAccuracy, validationLoss, validationAccuracy\n",
        "\n",
        "    # runs RMSPROP for one epoch\n",
        "    def oneEpochRMSPROP(self,epoch):\n",
        "        ''' Executes A Single Epoch for RMSPROP Gradient Descent Algorithm.\n",
        "            Returns the training loss,training accuracy,validaiton loss and validation accuracy,averaged over all points. \n",
        "        '''\n",
        "        n = self.train_n_samples\n",
        "        \n",
        "        # randomizing batches\n",
        "        idx = np.random.permutation(self.train_n_samples)\n",
        "        self.x_train = self.x_train[idx]\n",
        "        self.y_train = self.y_train[idx]\n",
        "\n",
        "        input = self.x_train\n",
        "        actual_y = self.y_train\n",
        "\n",
        "\n",
        "        # maintaining previous history for weights and bias\n",
        "        prev_wHistory, prev_bHistory = self.wHistory, self.bHistory\n",
        "\n",
        "        # Total Loss for epoch\n",
        "        loss_input = 0\n",
        "        beta = self.parameters[\"beta\"]\n",
        "        eps = self.parameters[\"epsilon\"]\n",
        "        count = 0\n",
        "        \n",
        "        # set accumalated gradients to zero\n",
        "        acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "\n",
        "        # execute one epoch for all datapoints in train set\n",
        "        for index in range(n):\n",
        "\n",
        "            # perfrom forward propagation\n",
        "            self.forward_propagation(input, index)\n",
        "            predicted_y = self.post_activation[\"h\" + str(self.L)]\n",
        "\n",
        "            # perform backward propagation\n",
        "            grad_weights, grad_bias = self.backward_propagation(index,actual_y)\n",
        "\n",
        "            # compute loss\n",
        "            loss_input += self.loss_function(predicted_y, self.y_train[index])\n",
        "\n",
        "            # compute the number of datapoints which are correctly classified\n",
        "            indexWithMaxProb = np.argmax(predicted_y)\n",
        "            if (actual_y[index] == (indexWithMaxProb)):\n",
        "                count = count + 1\n",
        "\n",
        "            # accumulate grad_weights and grad_bais for each input\n",
        "            for (key, value) in grad_weights.items():\n",
        "                acc_grad_weights[key] = acc_grad_weights[key] + grad_weights[key]\n",
        "\n",
        "            for (key, value) in grad_bias.items():\n",
        "                acc_grad_bias[key] = acc_grad_bias[key] + grad_bias[key]\n",
        "        \n",
        "\n",
        "            # update weights and bias if the number of datapoints in batch_size are divisble by batch_size\n",
        "            if ((index + 1) % self.batch_size == 0):\n",
        "    \n",
        "                # update weight history\n",
        "                for (key, value) in self.wHistory.items():\n",
        "                    self.wHistory[key] = beta * prev_wHistory[key] +  (1 - beta) * acc_grad_weights[key] ** 2\n",
        "\n",
        "                # update bias history\n",
        "                for (key, value) in self.bHistory.items():\n",
        "                    self.bHistory[key] = beta * prev_bHistory[key] + (1 - beta) * acc_grad_bias[key] ** 2\n",
        "\n",
        "                # update weights\n",
        "                for (key, value) in self.weights.items():\n",
        "                    self.weights[key] = self.weights[key] -  (self.learningRate / self.batch_size) * acc_grad_weights[key] / (np.sqrt(self.wHistory[key] + eps))\n",
        "\n",
        "                # update bias\n",
        "                for (key, value) in self.bias.items():\n",
        "                    self.bias[key] = self.bias[key] - (self.learningRate / self.batch_size) * acc_grad_bias[key] / (np.sqrt(self.bHistory[key] + eps))\n",
        "                \n",
        "                # updating histroy for weights and bias\n",
        "                self.prev_wHistory = self.wHistory\n",
        "                self.prev_bHistory = self.bHistory\n",
        "\n",
        "                # set accumalated gradients to zero\n",
        "                acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "\n",
        "        # if the number of datapoints in batch is not divisible by batch_size update weights and bias \n",
        "        if n % self.batch_size != 0:\n",
        "            \n",
        "            # update weight history\n",
        "            for (key, value) in self.wHistory.items():\n",
        "                self.wHistory[key] = beta * prev_wHistory[key] +  (1 - beta) * acc_grad_weights[key] ** 2\n",
        "\n",
        "            # update bias history\n",
        "            for (key, value) in self.bHistory.items():\n",
        "                self.bHistory[key] = beta * prev_bHistory[key] + (1 - beta) * acc_grad_bias[key] ** 2\n",
        "\n",
        "            # update weights\n",
        "            for (key, value) in self.weights.items():\n",
        "                self.weights[key] = self.weights[key] -  (self.learningRate / self.batch_size) * acc_grad_weights[key] / (np.sqrt(self.wHistory[key] + eps))\n",
        "\n",
        "            # update bias\n",
        "            for (key, value) in self.bias.items():\n",
        "                self.bias[key] = self.bias[key] - (self.learningRate / self.batch_size) * acc_grad_bias[key] / (np.sqrt(self.bHistory[key] + eps))\n",
        "            \n",
        "            # updating histroy for weights and bias\n",
        "            self.prev_wHistory = self.wHistory\n",
        "            self.prev_bHistory = self.bHistory\n",
        "\n",
        "            # set accumalated gradients to zero\n",
        "            acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "\n",
        "        # compute trainAccuracy,trainLoss averaged over train size\n",
        "        trainAccuracy = count / n\n",
        "        trainLoss = loss_input / n + self.regularize()\n",
        "\n",
        "        # compute validationAccuracy,validationLoss averaged over test size \n",
        "        validationLoss, validationAccuracy = self.computeTestLossAndAccuracy()\n",
        "        \n",
        "        return trainLoss, trainAccuracy, validationLoss, validationAccuracy\n",
        "\n",
        "    # runs ADAM gradient descent for one epoch\n",
        "    def oneEpochADAM(self,epoch):\n",
        "        ''' Executes A Single Epoch for ADAM Gradient Descent Algorithm.\n",
        "            Returns the training loss,training accuracy,validaiton loss and validation accuracy,averaged over all points. '''\n",
        "\n",
        "        n = self.train_n_samples\n",
        "        # randomizing batches\n",
        "        idx = np.random.permutation(self.train_n_samples)\n",
        "        self.x_train = self.x_train[idx]\n",
        "        self.y_train = self.y_train[idx]\n",
        "\n",
        "        input = self.x_train\n",
        "        actual_y = self.y_train\n",
        "\n",
        "        # maintaining previous history and momentum for weights and bias\n",
        "        prev_wMomentum, prev_bMomentum = self.wMomentum, self.bMomentum\n",
        "        prev_wHistory, prev_bHistory = self.wHistory, self.bHistory\n",
        "\n",
        "        wMomentum_hat = {} \n",
        "        bMomentum_hat = {}\n",
        "        wHistory_hat = {} \n",
        "        bHistory_hat = {}\n",
        "\n",
        "        # Total Loss for epoch\n",
        "        loss_input = 0\n",
        "        count = 0\n",
        "        beta1 = self.parameters[\"beta1\"]\n",
        "        beta2 = self.parameters[\"beta2\"]\n",
        "        epsilon = self.parameters[\"epsilon\"]\n",
        "        \n",
        "        # set accumalated gradients to zero\n",
        "        acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "\n",
        "        # execute one epoch for all datapoints in train set\n",
        "        for index in range(n):\n",
        "\n",
        "            # perform forward propagation\n",
        "\n",
        "            self.forward_propagation(input, index)\n",
        "            predicted_y = self.post_activation[\"h\" + str(self.L)]\n",
        "\n",
        "            # perform backward propagation\n",
        "            grad_weights, grad_bias = self.backward_propagation(index,actual_y)\n",
        "\n",
        "            # compute loss\n",
        "            loss_input += self.loss_function(predicted_y, actual_y[index])\n",
        "\n",
        "            # compute the number of datapoints which are correctly classified\n",
        "            indexWithMaxProb = np.argmax(predicted_y)\n",
        "            if (actual_y[index] == (indexWithMaxProb)):\n",
        "                count = count + 1\n",
        "\n",
        "            # accumulate grad_weights and grad_bais for each input\n",
        "            for (key, value) in grad_weights.items():\n",
        "                acc_grad_weights[key] = acc_grad_weights[key] + grad_weights[key]\n",
        "\n",
        "            for (key, value) in grad_bias.items():\n",
        "                acc_grad_bias[key] = acc_grad_bias[key] + grad_bias[key]\n",
        "\n",
        "            # update weights and bias if the number of datapoints in batch_size are divisble by batch_size\n",
        "            if ((index + 1) % self.batch_size == 0):\n",
        "        \n",
        "                # update weight momentum\n",
        "                for (key, value) in self.wMomentum.items():\n",
        "                    self.wMomentum[key] = beta1*prev_wMomentum[key] +  (1 - beta1) * acc_grad_weights[key]\n",
        "\n",
        "                # update bias momentum\n",
        "                for (key, value) in self.bMomentum.items():\n",
        "                    self.bMomentum[key] = beta1*prev_bMomentum[key] + (1 - beta1) * acc_grad_bias[key]\n",
        "\n",
        "                # update weight history\n",
        "                for (key, value) in self.wHistory.items():\n",
        "                    self.wHistory[key] = beta2 * prev_wHistory[key] + (1 - beta2) * acc_grad_weights[key] ** 2\n",
        "\n",
        "                # update bias history\n",
        "                for (key, value) in self.bHistory.items():\n",
        "                    self.bHistory[key] = beta2 * prev_bHistory[key] + (1 - beta2) * acc_grad_bias[key] ** 2\n",
        "\n",
        "                \n",
        "                # compute intermediate values\n",
        "                for (key, value) in self.weights.items():\n",
        "                    wMomentum_hat[key] = self.wMomentum[key] / (1 - np.power(beta1, epoch + 1))\n",
        "\n",
        "                for (key, value) in self.bias.items():\n",
        "                    bMomentum_hat[key] = self.bMomentum[key] / (1 - np.power(beta1, epoch + 1))\n",
        "\n",
        "                for (key, value) in self.weights.items():\n",
        "                    wHistory_hat[key] = self.wHistory[key] / (1 - np.power(beta2, epoch + 1))\n",
        "\n",
        "                for (key, value) in self.bias.items():\n",
        "                    bHistory_hat[key] = self.bHistory[key] / (1 - np.power(beta2, epoch + 1))\n",
        "\n",
        "                # update weights\n",
        "                for (key, value) in self.weights.items():\n",
        "                    temp = (self.learningRate / self.batch_size) * wMomentum_hat[key] / (np.sqrt(wHistory_hat[key] + epsilon))\n",
        "                    self.weights[key] = self.weights[key] - temp\n",
        "                \n",
        "                # update bias\n",
        "                for (key, value) in self.bias.items():\n",
        "                    temp = (self.learningRate / self.batch_size) * bMomentum_hat[key] / (np.sqrt(bHistory_hat[key] + epsilon))\n",
        "                    self.bias[key] = self.bias[key] - temp\n",
        "\n",
        "                # set accumalated gradients to zero\n",
        "                acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "            \n",
        "        # if the number of datapoints in batch is not divisible by batch_size update weights and bias \n",
        "        if n % self.batch_size != 0:\n",
        "            \n",
        "            # update weight momentum\n",
        "            for (key, value) in self.wMomentum.items():\n",
        "                self.wMomentum[key] = beta1*prev_wMomentum[key] +  (1 - beta1) * acc_grad_weights[key]\n",
        "\n",
        "            # update bias momentum\n",
        "            for (key, value) in self.bMomentum.items():\n",
        "                self.bMomentum[key] = beta1*prev_bMomentum[key] + (1 - beta1) * acc_grad_bias[key]\n",
        "\n",
        "            # update weight history\n",
        "            for (key, value) in self.wHistory.items():\n",
        "                self.wHistory[key] = beta2 * prev_wHistory[key] + (1 - beta2) * acc_grad_weights[key] ** 2\n",
        "\n",
        "            # update bias history\n",
        "            for (key, value) in self.bHistory.items():\n",
        "                self.bHistory[key] = beta2 * prev_bHistory[key] + (1 - beta2) * acc_grad_bias[key] ** 2\n",
        "\n",
        "            \n",
        "            # compute intermediate values\n",
        "            for (key, value) in self.weights.items():\n",
        "                wMomentum_hat[key] = self.wMomentum[key] / (1 - np.power(beta1, epoch + 1))\n",
        "\n",
        "            for (key, value) in self.bias.items():\n",
        "                bMomentum_hat[key] = self.bMomentum[key] / (1 - np.power(beta1, epoch + 1))\n",
        "\n",
        "            for (key, value) in self.weights.items():\n",
        "                wHistory_hat[key] = self.wHistory[key] / (1 - np.power(beta2, epoch + 1))\n",
        "\n",
        "            for (key, value) in self.bias.items():\n",
        "                bHistory_hat[key] = self.bHistory[key] / (1 - np.power(beta2, epoch + 1))\n",
        "\n",
        "            # update weights\n",
        "            for (key, value) in self.weights.items():\n",
        "                temp = (self.learningRate / self.batch_size) * wMomentum_hat[key] / (np.sqrt(wHistory_hat[key] + epsilon))\n",
        "                self.weights[key] = self.weights[key] - temp\n",
        "            \n",
        "            # update bias\n",
        "            for (key, value) in self.bias.items():\n",
        "                temp = (self.learningRate / self.batch_size) * bMomentum_hat[key] / (np.sqrt(bHistory_hat[key] + epsilon))\n",
        "                self.bias[key] = self.bias[key] - temp\n",
        "\n",
        "            # set accumalated gradients to zero\n",
        "            acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "        \n",
        "        # compute trainAccuracy,trainLoss averaged over train size\n",
        "        trainAccuracy = count / n\n",
        "        trainLoss = loss_input / n + self.regularize()\n",
        "        \n",
        "        # compute validationAccuracy,validationLoss averaged over test size \n",
        "        validationLoss, validationAccuracy = self.computeTestLossAndAccuracy()\n",
        "        \n",
        "        return trainLoss, trainAccuracy, validationLoss, validationAccuracy\n",
        "\n",
        "    # runs NADAM gradient descent for one epoch\n",
        "    def oneEpochNADAM(self,epoch):\n",
        "        ''' Executes A Single Epoch for NADAM Gradient Descent Algorithm.\n",
        "            Returns the training loss,training accuracy,validaiton loss and validation accuracy,averaged over all points. '''\n",
        "\n",
        "        n = self.train_n_samples\n",
        "        \n",
        "        # randomizing batches\n",
        "        idx = np.random.permutation(self.train_n_samples)\n",
        "        self.x_train = self.x_train[idx]\n",
        "        self.y_train = self.y_train[idx]\n",
        "\n",
        "        input = self.x_train\n",
        "        actual_y = self.y_train\n",
        "\n",
        "        # maintaining previous history and momentum for weights and bias\n",
        "        prev_wMomentum, prev_bMomentum = self.wMomentum, self.bMomentum\n",
        "        prev_wHistory, prev_bHistory = self.wHistory, self.bHistory\n",
        "\n",
        "        wMomentum_hat = {} \n",
        "        bMomentum_hat = {}\n",
        "        wHistory_hat = {} \n",
        "        bHistory_hat = {}\n",
        "\n",
        "        # Total Loss for epoch\n",
        "        loss_input = 0\n",
        "        count = 0\n",
        "        beta1 = self.parameters[\"beta1\"]\n",
        "        beta2 = self.parameters[\"beta2\"]\n",
        "        epsilon = self.parameters[\"epsilon\"]\n",
        "\n",
        "\n",
        "        # set accumalated gradients to zero\n",
        "        acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "\n",
        "        for index in range(n):\n",
        "\n",
        "            # perform forward propagation\n",
        "            self.forward_propagation(input, index)\n",
        "            predicted_y = self.post_activation[\"h\" + str(self.L)]\n",
        "\n",
        "            # perform backward propagation\n",
        "            grad_weights, grad_bias = self.backward_propagation(index,actual_y)\n",
        "\n",
        "            # compute loss\n",
        "            loss_input += self.loss_function(predicted_y, actual_y[index])\n",
        "\n",
        "            # compute the number of datapoints which are correctly classified\n",
        "            indexWithMaxProb = np.argmax(predicted_y)\n",
        "            if (actual_y[index] == (indexWithMaxProb)):\n",
        "                count = count + 1\n",
        "\n",
        "            # accumulate grad_weights and grad_bais for each input\n",
        "            for (key, value) in grad_weights.items():\n",
        "                acc_grad_weights[key] = acc_grad_weights[key] + grad_weights[key]\n",
        "\n",
        "            for (key, value) in grad_bias.items():\n",
        "                acc_grad_bias[key] = acc_grad_bias[key] + grad_bias[key]\n",
        "\n",
        "            # update weights and bias if the number of datapoints in batch_size are divisble by batch_size\n",
        "            if ((index + 1) % self.batch_size == 0):\n",
        "                \n",
        "                # update weight momentum\n",
        "                for (key, value) in self.wMomentum.items():\n",
        "                    self.wMomentum[key] = beta1*prev_wMomentum[key] + (1 - beta1) * acc_grad_weights[key]\n",
        "\n",
        "                # update bias momentum\n",
        "                for (key, value) in self.bMomentum.items():\n",
        "                    self.bMomentum[key] = beta1*prev_bMomentum[key] + (1 - beta1) * acc_grad_bias[key]\n",
        "\n",
        "                # update weight history\n",
        "                for (key, value) in self.wHistory.items():\n",
        "                    self.wHistory[key] = beta2 * prev_wHistory[key] + (1 - beta2) * acc_grad_weights[key] ** 2\n",
        "\n",
        "                # update bias history\n",
        "                for (key, value) in self.bHistory.items():\n",
        "                    self.bHistory[key] = beta2 * prev_bHistory[key] + (1 - beta2) * acc_grad_bias[key] ** 2\n",
        "\n",
        "\n",
        "                # compute intermediate values\n",
        "                for (key, value) in self.weights.items():\n",
        "                    wMomentum_hat[key] = self.wMomentum[key] / (1 - np.power(beta1, epoch + 1))\n",
        "                \n",
        "                for (key, value) in self.bias.items():\n",
        "                    bMomentum_hat[key] = self.bMomentum[key] / (1 - np.power(beta1, epoch + 1))\n",
        "\n",
        "                for (key, value) in self.weights.items():\n",
        "                    wHistory_hat[key] = self.wHistory[key] / (1 - np.power(beta2, epoch + 1))\n",
        "\n",
        "                for (key, value) in self.bias.items():\n",
        "                    bHistory_hat[key] = self.bHistory[key] / (1 - np.power(beta2, epoch + 1))\n",
        "\n",
        "                # update weights\n",
        "                for (key, value) in self.weights.items():\n",
        "                    num1 = ((self.learningRate / self.batch_size) / np.sqrt(wHistory_hat[key] + epsilon))\n",
        "                    num2 = beta1 * wMomentum_hat[key] + ((1 - beta1) * acc_grad_weights[key] / (1 - beta1 ** (epoch + 1)))\n",
        "                    self.weights[key] = self.weights[key] - num1*num2\n",
        "\n",
        "                # update bias\n",
        "                for (key, value) in self.bias.items():\n",
        "                    num1 = ((self.learningRate / self.batch_size) / np.sqrt(bHistory_hat[key] + epsilon))\n",
        "                    num2 = beta1 * bMomentum_hat[key] + ((1 - beta1) * acc_grad_bias[key] / (1 - beta1 ** (epoch + 1)))\n",
        "                    self.bias[key] = self.bias[key] - num1*num2\n",
        "\n",
        "                # set accumalated gradients to zero\n",
        "                acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "            \n",
        "            # if the number of datapoints in batch is not divisible by batch_size update weights and bias \n",
        "        if n % self.batch_size != 0:\n",
        "            \n",
        "            # update weight momentum\n",
        "            for (key, value) in self.wMomentum.items():\n",
        "                self.wMomentum[key] = beta1*prev_wMomentum[key] + (1 - beta1) * acc_grad_weights[key]\n",
        "\n",
        "            # update bias momentum\n",
        "            for (key, value) in self.bMomentum.items():\n",
        "                self.bMomentum[key] = beta1*prev_bMomentum[key] + (1 - beta1) * acc_grad_bias[key]\n",
        "\n",
        "            # update weight history\n",
        "            for (key, value) in self.wHistory.items():\n",
        "                self.wHistory[key] = beta2 * prev_wHistory[key] + (1 - beta2) * acc_grad_weights[key] ** 2\n",
        "\n",
        "            # update bias history\n",
        "            for (key, value) in self.bHistory.items():\n",
        "                self.bHistory[key] = beta2 * prev_bHistory[key] + (1 - beta2) * acc_grad_bias[key] ** 2\n",
        "\n",
        "\n",
        "            # compute intermediate values\n",
        "            for (key, value) in self.weights.items():\n",
        "                wMomentum_hat[key] = self.wMomentum[key] / (1 - np.power(beta1, epoch + 1))\n",
        "            \n",
        "            for (key, value) in self.bias.items():\n",
        "                bMomentum_hat[key] = self.bMomentum[key] / (1 - np.power(beta1, epoch + 1))\n",
        "\n",
        "            for (key, value) in self.weights.items():\n",
        "                wHistory_hat[key] = self.wHistory[key] / (1 - np.power(beta2, epoch + 1))\n",
        "\n",
        "            for (key, value) in self.bias.items():\n",
        "                bHistory_hat[key] = self.bHistory[key] / (1 - np.power(beta2, epoch + 1))\n",
        "\n",
        "            # update weights\n",
        "            for (key, value) in self.weights.items():\n",
        "                num1 = ((self.learningRate / self.batch_size) / np.sqrt(wHistory_hat[key] + epsilon))\n",
        "                num2 = beta1 * wMomentum_hat[key] + ((1 - beta1) * acc_grad_weights[key] / (1 - beta1 ** (epoch + 1)))\n",
        "                self.weights[key] = self.weights[key] - num1*num2\n",
        "\n",
        "            # update bias\n",
        "            for (key, value) in self.bias.items():\n",
        "                num1 = ((self.learningRate / self.batch_size) / np.sqrt(bHistory_hat[key] + epsilon))\n",
        "                num2 = beta1 * bMomentum_hat[key] + ((1 - beta1) * acc_grad_bias[key] / (1 - beta1 ** (epoch + 1)))\n",
        "                self.bias[key] = self.bias[key] - num1*num2\n",
        "\n",
        "            # set accumalated gradients to zero\n",
        "            acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "        \n",
        "                \n",
        "        # compute trainAccuracy,trainLoss averaged over train size\n",
        "        trainAccuracy = count / n\n",
        "        trainLoss = loss_input / n + self.regularize()\n",
        "        \n",
        "        # compute validationAccuracy,validationLoss averaged over test size \n",
        "        validationLoss, validationAccuracy = self.computeTestLossAndAccuracy()\n",
        "        return trainLoss, trainAccuracy, validationLoss, validationAccuracy\n",
        "\n",
        "    # runs gradient descent for one epoch\n",
        "    def oneEpochGD(self, epoch):\n",
        "        ''' Executes A Single Epoch for Vanilla Gradient Descent Algorithm.\n",
        "            Returns the training loss,training accuracy,validaiton loss and validation accuracy,averaged over all points. '''\n",
        "\n",
        "        n = self.train_n_samples\n",
        "        \n",
        "        # randomizing batches\n",
        "        idx = np.random.permutation(self.train_n_samples)\n",
        "        self.x_train = self.x_train[idx]\n",
        "        self.y_train = self.y_train[idx]\n",
        "\n",
        "        input = self.x_train\n",
        "        actual_y = self.y_train\n",
        "\n",
        "        # Total Loss for epoch\n",
        "        loss_input = 0\n",
        "        count = 0\n",
        "\n",
        "        # set accumalated gradients to zero\n",
        "        acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "\n",
        "        # execute one epoch for all datapoints in train set\n",
        "        for index in range(n):\n",
        "\n",
        "            # perform forward propagation\n",
        "            self.forward_propagation(input, index)\n",
        "            predicted_y = self.post_activation[\"h\" + str(self.L)]\n",
        "\n",
        "            # compute loss\n",
        "            loss_input += self.loss_function(predicted_y, actual_y[index])\n",
        "\n",
        "            # perofrm backward propagation\n",
        "            grad_weights, grad_bias = self.backward_propagation(index, actual_y)\n",
        "\n",
        "            # compute the number of datapoints which are correctly classified\n",
        "            indexWithMaxProb = np.argmax(predicted_y)\n",
        "            if (actual_y[index] == (indexWithMaxProb)):\n",
        "                count = count + 1\n",
        "\n",
        "            # accumulate grad_weights and grad_bais for each input\n",
        "            for (key, value) in grad_weights.items():\n",
        "                acc_grad_weights[key] = acc_grad_weights[key] + \\\n",
        "                    grad_weights[key]\n",
        "\n",
        "            for (key, value) in grad_bias.items():\n",
        "                acc_grad_bias[key] = acc_grad_bias[key] + grad_bias[key]\n",
        "\n",
        "            # update weights and bias if the number of datapoints in batch_size are divisble by batch_size\n",
        "            if ((index + 1) % self.batch_size == 0):\n",
        "\n",
        "                # update weights\n",
        "                for (key, value) in self.weights.items():\n",
        "                    self.weights[key] = self.weights[key] - ((self.learningRate / self.batch_size) * acc_grad_weights[key])\n",
        "                \n",
        "                # update bias\n",
        "                for (key, value) in self.bias.items():\n",
        "                    self.bias[key] = self.bias[key] - ((self.learningRate / self.batch_size) * acc_grad_bias[key])\n",
        "\n",
        "                # set accumalated gradients to zero\n",
        "                acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "\n",
        "        # if the number of datapoints in batch is not divisible by batch_size update weights and bias \n",
        "        if n % self.batch_size != 0:\n",
        "\n",
        "            # update weights\n",
        "            for (key, value) in self.weights.items():\n",
        "                self.weights[key] = self.weights[key] - ((self.learningRate / self.batch_size) * acc_grad_weights[key])\n",
        "            \n",
        "            # update bias\n",
        "            for (key, value) in self.bias.items():\n",
        "                self.bias[key] = self.bias[key] - ((self.learningRate / self.batch_size) * acc_grad_bias[key])\n",
        "\n",
        "            # set accumalated gradients to zero\n",
        "            acc_grad_weights, acc_grad_bias = self.make_accumalate_zero()\n",
        "\n",
        "        # compute trainAccuracy,trainLoss averaged over train size\n",
        "        trainAccuracy = count / n\n",
        "        trainLoss = loss_input / n + self.regularize()\n",
        "        \n",
        "        # compute validationAccuracy,validationLoss averaged over test size \n",
        "        validationLoss, validationAccuracy = self.computeTestLossAndAccuracy()\n",
        "        \n",
        "        return trainLoss, trainAccuracy, validationLoss, validationAccuracy\n",
        "\n",
        "    # executes a single epoch of the FeedForward NN according to the optimizer function. \n",
        "    def executeOneEpoch(self,epoch):\n",
        "        \n",
        "        if self.optimizer == \"sgd\":\n",
        "            return self.oneEpochSGD(epoch)\n",
        "\n",
        "        if self.optimizer == \"momentum\":\n",
        "            return self.oneEpochMOMENTUM(epoch)\n",
        "\n",
        "        if self.optimizer == \"nestrov\":\n",
        "            return self.oneEpochNAG(epoch)\n",
        "\n",
        "        if self.optimizer == \"rmsprop\":\n",
        "            return self.oneEpochRMSPROP(epoch)\n",
        "\n",
        "        if self.optimizer == \"adam\":\n",
        "            return self.oneEpochADAM(epoch)\n",
        "\n",
        "        if self.optimizer == \"nadam\":\n",
        "            return self.oneEpochNADAM(epoch)\n",
        "\n",
        "        if self.optimizer == \"gd\":\n",
        "            return self.oneEpochGD(epoch)\n",
        "\n",
        "    # computes validation loss and validation accuracy \n",
        "    def computeTestLossAndAccuracy(self):\n",
        "        \n",
        "        validation_size = self.y_validate.shape[0]\n",
        "        test_loss = 0\n",
        "        count = 0\n",
        "\n",
        "        input = self.x_validate\n",
        "        actual_y = self.y_validate\n",
        "\n",
        "        for index in range(0, validation_size):\n",
        "\n",
        "            # perform forward propagation\n",
        "            self.forward_propagation(input, index)\n",
        "            predicted_y = self.post_activation[\"h\" + str(self.L)]\n",
        "\n",
        "            # compute loss\n",
        "            test_loss += self.loss_function(predicted_y, actual_y[index])\n",
        "\n",
        "            # compute the number of datapoints which are correctly classified\n",
        "            indexWithMaxProb = np.argmax(predicted_y)\n",
        "            if (actual_y[index] == (indexWithMaxProb)):\n",
        "                count = count + 1\n",
        "\n",
        "        # compute validationAccuracy,validationLoss averaged over validation size \n",
        "        validationAccuracy = count / validation_size\n",
        "        validationLoss = test_loss / validation_size + self.regularize()\n",
        "        \n",
        "        return validationLoss, validationAccuracy\n",
        "\n",
        "    '''<----------------------------Question 1------------------------------------->'''\n",
        "    def question_1(self):\n",
        "\n",
        "        wandb.init(\n",
        "                # set the wandb project where this run will be logged\n",
        "                project = feed_forward.parameters[\"wandb_project\"],\n",
        "                # config = sweep_config\n",
        "        )\n",
        "\n",
        "        # dictionary of labels to be added\n",
        "        labels_added = {}\n",
        "\n",
        "        ''' \n",
        "        Running the loop for the number of training samples.\n",
        "        In each iteration,a random index is generated and we extract the feature and label at the generated index.\n",
        "        If the label is already in the labels_added dictionary,we ignore that label,else we add that (label,feature) \n",
        "        as (key,value) pair in dictionary (so that one label is considered only once).\n",
        "        '''\n",
        "        images = []\n",
        "        for i in range(self.train_n_samples):\n",
        "            index = random.randrange(self.train_n_samples)\n",
        "            feature = self. x_train[index]\n",
        "            label = self.y_train[index]\n",
        "            if (label in labels_added.keys()):\n",
        "                continue\n",
        "            labels_added[label] = feature\n",
        "            image = wandb.Image(\n",
        "                labels_added[label], caption=f\"{self.title[label]}\")\n",
        "            images.append(image)\n",
        "        wandb.log({\"Images\": images})\n",
        "    \n",
        "    '''<----------------------------Question 2------------------------>'''\n",
        "\n",
        "    def feed_forward_q2(self): \n",
        "        # initialization of weights\n",
        "        self.weightsAndBiasInitializer()\n",
        " \n",
        "        # train the data\n",
        "        for i in range(1, self.epoch + 1):\n",
        "            (train_Loss, train_Accuracy, validation_Loss,\n",
        "             validation_Accuracy) = self.executeOneEpoch(i)\n",
        "            \n",
        "        # generating the random index to test the model and finding the y for that\n",
        "        index = np.random.randint(self.y_validate.shape[0])\n",
        "        input = self.x_validate\n",
        "        self.forward_propagation(input, index)\n",
        "        predicted_y = self.post_activation[\"h\" + str(self.L)]\n",
        "        print(predicted_y)\n",
        "\n",
        "    '''<----------------------------Question 3-4------------------------------------->'''\n",
        "    def feed_forward_q3_4(self):\n",
        "        \n",
        "        self.weights = dict()\n",
        "        self.bias = dict()\n",
        "        self.wHistory = dict()\n",
        "        self.bHistory = dict()\n",
        "        self.wMomentum = dict()\n",
        "        self.bMomentum = dict()\n",
        "        \n",
        "        # initialization of weights and bias\n",
        "        self.weightsAndBiasInitializer()\n",
        "\n",
        "        # initializing history for weights and bias\n",
        "        self.historyInitializer()\n",
        "\n",
        "        # initializing momentum for weights and bias\n",
        "        self.momentumInitializer()\n",
        "\n",
        "        self.validation_Accuracy = 0\n",
        "\n",
        "\n",
        "        # run feedforward NN \n",
        "        for i in range(1, self.epoch + 1):\n",
        "              (train_Loss, train_Accuracy, validation_Loss,self.validation_Accuracy) = self.executeOneEpoch(i)\n",
        "              print(\"epoch:{epoch}, train loss:{train_l}, train accuracy:{train_ac}, validation loss:{validation_l}, validation accuracy:{validation_ac}\".\\\n",
        "                  format(epoch = i,train_l = train_Loss,train_ac = train_Accuracy,validation_l = validation_Loss,validation_ac = self.validation_Accuracy))\n",
        "            \n",
        "              wandb.log({'train loss':train_Loss, 'train accuracy':train_Accuracy,'validation loss':validation_Loss, 'validation accuracy':self.validation_Accuracy})\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxlBlgWiCWJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf87a44-904f-497d-f1fd-f56fd64bf4b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "feed_forward = FeedForward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "I1CcpV4d6z2c",
        "outputId": "d4da7efd-4c04-4201-edda-27779b0b6ce9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: bv35yzaw\n",
            "Sweep URL: https://wandb.ai/cs22m019/DL%20Final%20Assignment%201/sweeps/bv35yzaw\n"
          ]
        }
      ],
      "source": [
        "sweep_config = {\n",
        "\n",
        "        'method' : 'random', #grid ,random - generates exponential ways,bayesian  efficient way\n",
        "        'name' : 'random_sweep mse',\n",
        "        'metric' : {\n",
        "            'name' : 'validation accuracy',\n",
        "            'goal' : 'maximize'\n",
        "        },\n",
        "        'parameters':{\n",
        "                'epochs' : {\n",
        "                    'values' : [5,10]\n",
        "                },\n",
        "                'number_of_hidden_layer':{\n",
        "                    'values' : [3,4,5]\n",
        "                },\n",
        "                'size_of_hidden_layer' : {\n",
        "                    'values' :[32,64,128]\n",
        "                },\n",
        "                'weight_decay' : {\n",
        "                    'values' : [0,0.0005,0.5]\n",
        "                },\n",
        "                'learning_rate' : {\n",
        "                    'values' : [1e-3,1e-4]\n",
        "                },\n",
        "                'optimizer' : {\n",
        "                    'values' : ['sgd','momentum','nestrov','rmsprop','adam','nadam']\n",
        "                },\n",
        "                'batch_size' : {\n",
        "                        'values' : [16,32,64]\n",
        "                },\n",
        "                'weight_initialization' :{\n",
        "                    'values' : ['random','Xavier']\n",
        "                },\n",
        "                'activation' : {\n",
        "                    'values' : ['sigmoid','tanh','ReLU']\n",
        "                }\n",
        "        }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep = sweep_config,project= feed_forward.parameters[\"wandb_project\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whziGn3a62Pv"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    wandb.init(\n",
        "                # set the wandb project where this run will be logged\n",
        "                # project = feed_forward.parameters[\"wandb_project\"],\n",
        "                config = sweep_config\n",
        "    )\n",
        "    \n",
        "    feed_forward.epoch = wandb.config.epochs\n",
        "    feed_forward.nnl = wandb.config.size_of_hidden_layer\n",
        "    feed_forward.weightDecay =  wandb.config.weight_decay\n",
        "    feed_forward.learningRate = wandb.config.learning_rate\n",
        "    feed_forward.optimizer = wandb.config.optimizer\n",
        "    feed_forward.batch_size = wandb.config.batch_size\n",
        "    feed_forward.weightInitialization = wandb.config.weight_initialization\n",
        "    feed_forward.activationFunction = wandb.config.activation\n",
        "    feed_forward.L = wandb.config.number_of_hidden_layer + 1\n",
        "    feed_forward.weight_decay = wandb.config.weight_decay\n",
        "\n",
        "\n",
        "    wandb.run.name = \"optimizer_\" + str(wandb.config.optimizer) +  \"_hl_\"+ str(wandb.config.number_of_hidden_layer) + \"_bs_\" + str(wandb.config.batch_size) + \"_ac_\" + str(wandb.config.activation)    \n",
        "    feed_forward.feed_forward_q3_4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEush2kM64C9"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id=sweep_id,function = train,count = 100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}